"""
ë„¤ì´ë²„ ìƒí’ˆëª… ìƒì„±ê¸° ì›Œì»¤
ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ ë° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
"""
from typing import List, Dict, Any, Optional
from PySide6.QtCore import QThread, Signal, QMutex, QMutexLocker
import time

from src.foundation.logging import get_logger
from src.toolbox.progress import calc_percentage

from .models import (
    AnalysisStep, KeywordBasicData, ProductNameData, AIAnalysisResult, GeneratedTitle
)
from .adapters import parse_keywords, collect_product_names_for_keywords

logger = get_logger("features.naver_product_title_generator.worker")


class BasicAnalysisWorker(QThread):
    """2ë‹¨ê³„: ê¸°ì´ˆë¶„ì„ ì›Œì»¤"""
    
    # ì‹œê·¸ë„ ì •ì˜
    progress_updated = Signal(int, str)  # progress%, message
    analysis_completed = Signal(list)    # List[KeywordBasicData]
    error_occurred = Signal(str)         # error_message
    
    def __init__(self, product_name: str):
        super().__init__()
        self.product_name = product_name
        self._stop_requested = False
        self._mutex = QMutex()
    
    def request_stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­"""
        with QMutexLocker(self._mutex):
            self._stop_requested = True
            
    def stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ (í•˜ìœ„ í˜¸í™˜)"""
        self.request_stop()
    
    def is_stopped(self) -> bool:
        """ì¤‘ë‹¨ ìš”ì²­ í™•ì¸"""
        with QMutexLocker(self._mutex):
            return self._stop_requested
    
    def run(self):
        """ì›Œì»¤ ì‹¤í–‰"""
        try:
            logger.info(f"ê¸°ì´ˆë¶„ì„ ì‹œì‘: {self.product_name}")
            
            # 1ë‹¨ê³„: í‚¤ì›Œë“œ íŒŒì‹±
            self.progress_updated.emit(0, "í‚¤ì›Œë“œ íŒŒì‹± ì¤‘...")
            
            if self.is_stopped():
                return
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = parse_keywords(self.product_name)
            
            if not keywords:
                self.error_occurred.emit("ë¶„ì„í•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(20, f"{len(keywords)}ê°œ í‚¤ì›Œë“œ íŒŒì‹± ì™„ë£Œ")
            
            # 2ë‹¨ê³„: í‚¤ì›Œë“œë³„ ì›”ê²€ìƒ‰ëŸ‰ ë° ì¹´í…Œê³ ë¦¬ ë¶„ì„
            self.progress_updated.emit(30, "ë„¤ì´ë²„ API ë¶„ì„ ì¤‘...")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from src.features.keyword_analysis.service import KeywordAnalysisService
            from src.features.keyword_analysis.models import AnalysisPolicy, AnalysisScope
            from .models import KeywordBasicData
            
            # ì •ì±… ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ì „ì²´ ë¶„ì„)
            policy = AnalysisPolicy(scope=AnalysisScope.FULL_ANALYSIS)
            analyzed_keywords = []
            
            # í‚¤ì›Œë“œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° (3ê°œì”© ë³‘ë ¬ ì²˜ë¦¬)
            def analyze_keyword_batch(keywords_batch, start_index):
                """í‚¤ì›Œë“œ ë°°ì¹˜ ë¶„ì„ (ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… í¬í•¨)"""
                from src.foundation.http_client import rate_limiter_manager
                
                analysis_service = KeywordAnalysisService(policy)  # ìŠ¤ë ˆë“œë³„ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
                results = []
                
                # ë„¤ì´ë²„ API ì „ìš© ë ˆì´íŠ¸ ë¦¬ë¯¸í„° (ì´ˆë‹¹ 2íšŒ í˜¸ì¶œ)
                rate_limiter = rate_limiter_manager.get_limiter("naver_searchad", calls_per_second=2.0)
                
                for i, keyword in enumerate(keywords_batch):
                    if self.is_stopped():
                        break
                    
                    # ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ì ìš©
                    with rate_limiter:
                        try:
                            logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘: '{keyword}' ({start_index + i + 1}/{len(keywords)})")
                            kw_data = analysis_service.analyze_single_keyword(keyword)
                            
                            # KeywordDataë¥¼ KeywordBasicDataë¡œ ë³€í™˜ (ì¹´í…Œê³ ë¦¬ëŠ” ì²« ë²ˆì§¸ ì¤„ë§Œ ì‚¬ìš©)
                            category = kw_data.category.split('\n')[0] if kw_data.category else ""
                            basic_data = KeywordBasicData(
                                keyword=kw_data.keyword,
                                search_volume=kw_data.search_volume,
                                category=category
                            )
                            results.append(basic_data)
                            
                            if kw_data.search_volume and kw_data.search_volume > 0:
                                logger.info(f"âœ… '{keyword}' ë¶„ì„ ì™„ë£Œ: ì›”ê²€ìƒ‰ëŸ‰ {kw_data.search_volume}")
                            else:
                                logger.debug(f"âŒ '{keyword}' ë¶„ì„ ì™„ë£Œ: ì›”ê²€ìƒ‰ëŸ‰ 0")
                                
                        except Exception as e:
                            logger.warning(f"âŒ í‚¤ì›Œë“œ '{keyword}' ë¶„ì„ ì‹¤íŒ¨: {e}")
                            # ì‹¤íŒ¨í•œ í‚¤ì›Œë“œë„ í¬í•¨ (ê²€ìƒ‰ëŸ‰ 0)
                            results.append(KeywordBasicData(
                                keyword=keyword,
                                search_volume=0,
                                category="ë¶„ì„ ì‹¤íŒ¨"
                            ))
                
                return results
            
            # í‚¤ì›Œë“œë¥¼ 3ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
            batch_size = 3
            total_keywords = len(keywords)
            batches = []
            
            for i in range(0, total_keywords, batch_size):
                batch = keywords[i:i + batch_size]
                batches.append((batch, i))
            
            logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(batches)}ê°œ ë°°ì¹˜, ë°°ì¹˜ë‹¹ {batch_size}ê°œ í‚¤ì›Œë“œ")
            self.progress_updated.emit(25, f"{total_keywords}ê°œ í‚¤ì›Œë“œë¥¼ 3ê°œì”© ë°°ì¹˜ë¡œ ë³‘ë ¬ ë¶„ì„ ì‹œì‘...")
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 3ê°œ ìŠ¤ë ˆë“œ)
            with ThreadPoolExecutor(max_workers=3) as executor:
                # ëª¨ë“  ë°°ì¹˜ë¥¼ ì œì¶œ
                future_to_batch = {
                    executor.submit(analyze_keyword_batch, batch, start_idx): (batch, start_idx)
                    for batch, start_idx in batches
                }
                
                completed_count = 0
                for future in as_completed(future_to_batch):
                    if self.is_stopped():
                        # ëª¨ë“  ì‘ì—… ì·¨ì†Œ
                        for f in future_to_batch:
                            f.cancel()
                        break
                    
                    batch, start_idx = future_to_batch[future]
                    
                    try:
                        batch_results = future.result()
                        analyzed_keywords.extend(batch_results)
                        
                        completed_count += len(batch)
                        progress = 30 + int((completed_count / total_keywords) * 60)  # 30% ~ 90%
                        
                        # í˜„ì¬ ë°°ì¹˜ì˜ í‚¤ì›Œë“œë“¤ì„ ì§„í–‰ë¥  ë©”ì‹œì§€ì— í¬í•¨
                        batch_keywords = [kw for kw in batch]  # í˜„ì¬ ì²˜ë¦¬ëœ ë°°ì¹˜ì˜ í‚¤ì›Œë“œë“¤
                        if len(batch_keywords) == 1:
                            current_keyword = batch_keywords[0]
                        else:
                            current_keyword = f"{batch_keywords[0]} ì™¸ {len(batch_keywords)-1}ê°œ"
                        
                        self.progress_updated.emit(
                            progress, 
                            f"'{current_keyword}' ë¶„ì„ ì™„ë£Œ ({completed_count}/{total_keywords})"
                        )
                        
                        logger.info(f"ë°°ì¹˜ ì™„ë£Œ: {len(batch_results)}ê°œ í‚¤ì›Œë“œ ë¶„ì„ë¨ - {completed_keywords}")
                        
                    except Exception as e:
                        logger.error(f"ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
                        # ì‹¤íŒ¨í•œ ë°°ì¹˜ì˜ í‚¤ì›Œë“œë“¤ì„ 0 ê²€ìƒ‰ëŸ‰ìœ¼ë¡œ ì¶”ê°€
                        for keyword in batch:
                            analyzed_keywords.append(KeywordBasicData(
                                keyword=keyword,
                                search_volume=0,
                                category="ë¶„ì„ ì‹¤íŒ¨"
                            ))
            
            # ê²°ê³¼ ì •ë ¬ (ì›ë˜ ìˆœì„œ ìœ ì§€)
            keyword_order = {kw: i for i, kw in enumerate(keywords)}
            analyzed_keywords.sort(key=lambda x: keyword_order.get(x.keyword, 999999))
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(90, "í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ")
            
            # ê²€ìƒ‰ëŸ‰ì´ 0ë³´ë‹¤ í° í‚¤ì›Œë“œë§Œ í•„í„°ë§
            valid_keywords = [kw for kw in analyzed_keywords if kw.search_volume > 0]
            
            if not valid_keywords:
                # ê²€ìƒ‰ëŸ‰ì´ ì—†ì–´ë„ ëª¨ë“  í‚¤ì›Œë“œ ë°˜í™˜ (ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆë„ë¡)
                valid_keywords = analyzed_keywords
            
            self.progress_updated.emit(100, f"ë¶„ì„ ì™„ë£Œ: {len(valid_keywords)}ê°œ í‚¤ì›Œë“œ")
            
            # ì™„ë£Œ ì‹œê·¸ë„ ë°œì†¡
            self.analysis_completed.emit(valid_keywords)
            
            logger.info(f"ê¸°ì´ˆë¶„ì„ ì™„ë£Œ: {len(valid_keywords)}ê°œ í‚¤ì›Œë“œ")
            
        except Exception as e:
            logger.error(f"ê¸°ì´ˆë¶„ì„ ì‹¤íŒ¨: {e}")
            self.error_occurred.emit(f"ê¸°ì´ˆë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


class ProductNameCollectionWorker(QThread):
    """2ë‹¨ê³„: ìƒí’ˆëª… ìˆ˜ì§‘ ì›Œì»¤"""
    
    # ì‹œê·¸ë„ ì •ì˜
    progress_updated = Signal(int, str)  # progress%, message
    collection_completed = Signal(list)  # List[Dict] - ìƒí’ˆëª… ë°ì´í„°
    error_occurred = Signal(str)         # error_message
    
    def __init__(self, selected_keywords: List[KeywordBasicData]):
        super().__init__()
        self.selected_keywords = selected_keywords
        self._stop_requested = False
        self._mutex = QMutex()
    
    def request_stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­"""
        with QMutexLocker(self._mutex):
            self._stop_requested = True
            
    def stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ (í•˜ìœ„ í˜¸í™˜)"""
        self.request_stop()
    
    def is_stopped(self) -> bool:
        """ì¤‘ë‹¨ ìš”ì²­ í™•ì¸"""
        with QMutexLocker(self._mutex):
            return self._stop_requested
    
    def run(self):
        """ì›Œì»¤ ì‹¤í–‰"""
        try:
            logger.info(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì‹œì‘: {len(self.selected_keywords)}ê°œ í‚¤ì›Œë“œ")
            
            if not self.selected_keywords:
                self.error_occurred.emit("ì„ íƒëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # í‚¤ì›Œë“œ ë¬¸ìì—´ ì¶”ì¶œ
            keywords = [kw.keyword for kw in self.selected_keywords]
            
            self.progress_updated.emit(10, f"{len(keywords)}ê°œ í‚¤ì›Œë“œë¡œ ìƒí’ˆëª… ìˆ˜ì§‘ ì‹œì‘...")
            
            if self.is_stopped():
                return
            
            # ê° í‚¤ì›Œë“œë³„ë¡œ ìƒí’ˆëª… ìˆ˜ì§‘ (ì§„í–‰ë¥  ì—…ë°ì´íŠ¸)
            total_keywords = len(keywords)
            collected_data = []
            
            for i, keyword in enumerate(keywords):
                if self.is_stopped():
                    return
                
                progress = 20 + int((i / total_keywords) * 60)  # 20~80%
                self.progress_updated.emit(progress, f"{keyword} ìƒí’ˆëª… ìˆ˜ì§‘ ì¤‘...")
                
                # í‚¤ì›Œë“œë³„ ìƒí’ˆëª… ìˆ˜ì§‘
                try:
                    keyword_products = collect_product_names_for_keywords([keyword], 40)
                    collected_data.extend(keyword_products)
                    
                    # ì§§ì€ ëŒ€ê¸° (API ê³¼ë¶€í•˜ ë°©ì§€)
                    time.sleep(0.2)
                    
                except Exception as e:
                    logger.warning(f"í‚¤ì›Œë“œ {keyword} ìƒí’ˆëª… ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    continue
                
                if self.is_stopped():
                    return
            
            self.progress_updated.emit(85, "ì¤‘ë³µ ì œê±° ì¤‘...")
            
            if self.is_stopped():
                return
            
            # ì „ì²´ ì¤‘ë³µ ì œê±°
            final_products = collect_product_names_for_keywords(keywords, 40)
            
            self.progress_updated.emit(100, f"ìƒí’ˆëª… ìˆ˜ì§‘ ì™„ë£Œ: {len(final_products)}ê°œ")
            
            # ì™„ë£Œ ì‹œê·¸ë„ ë°œì†¡
            self.collection_completed.emit(final_products)
            
            logger.info(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì™„ë£Œ: {len(final_products)}ê°œ")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.error_occurred.emit(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")



class WorkerManager:
    """ì›Œì»¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.current_worker: Optional[QThread] = None
        self.worker_history = []
    
    def start_worker(self, worker: QThread) -> bool:
        """ìƒˆ ì›Œì»¤ ì‹œì‘"""
        try:
            # ê¸°ì¡´ ì›Œì»¤ê°€ ìˆìœ¼ë©´ ì •ë¦¬
            self.stop_current_worker()
            
            # ìƒˆ ì›Œì»¤ ì‹œì‘
            self.current_worker = worker
            self.worker_history.append(worker)
            worker.start()
            
            logger.info(f"ì›Œì»¤ ì‹œì‘: {worker.__class__.__name__}")
            return True
            
        except Exception as e:
            logger.error(f"ì›Œì»¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False
    
    def stop_current_worker(self) -> bool:
        """í˜„ì¬ ì›Œì»¤ ì¤‘ë‹¨"""
        if self.current_worker and self.current_worker.isRunning():
            try:
                # ì›Œì»¤ì— ì¤‘ë‹¨ ìš”ì²­
                if hasattr(self.current_worker, 'stop'):
                    self.current_worker.stop()
                
                # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
                if not self.current_worker.wait(5000):
                    logger.warning("ì›Œì»¤ê°€ 5ì´ˆ ë‚´ì— ì¢…ë£Œë˜ì§€ ì•ŠìŒ, ê°•ì œ ì¢…ë£Œ")
                    self.current_worker.terminate()
                    self.current_worker.wait(2000)
                
                logger.info(f"ì›Œì»¤ ì¤‘ë‹¨ ì™„ë£Œ: {self.current_worker.__class__.__name__}")
                return True
                
            except Exception as e:
                logger.error(f"ì›Œì»¤ ì¤‘ë‹¨ ì‹¤íŒ¨: {e}")
                return False
        
        return True
        
    def stop_worker(self, worker: QThread) -> bool:
        """íŠ¹ì • ì›Œì»¤ ì¤‘ë‹¨"""
        if worker and worker.isRunning():
            try:
                # ì›Œì»¤ì— ì¤‘ë‹¨ ìš”ì²­
                if hasattr(worker, 'request_stop'):
                    worker.request_stop()
                elif hasattr(worker, 'stop'):
                    worker.stop()
                
                # ìµœëŒ€ 3ì´ˆ ëŒ€ê¸°
                if not worker.wait(3000):
                    logger.warning(f"ì›Œì»¤ê°€ 3ì´ˆ ë‚´ì— ì¢…ë£Œë˜ì§€ ì•ŠìŒ: {worker.__class__.__name__}")
                    worker.terminate()
                    worker.wait(1000)
                
                logger.info(f"ì›Œì»¤ ì¤‘ë‹¨ ì™„ë£Œ: {worker.__class__.__name__}")
                return True
                
            except Exception as e:
                logger.error(f"ì›Œì»¤ ì¤‘ë‹¨ ì‹¤íŒ¨: {e}")
                return False
        
        return True
    
    def cleanup_all_workers(self):
        """ëª¨ë“  ì›Œì»¤ ì •ë¦¬"""
        self.stop_current_worker()
        
        # íˆìŠ¤í† ë¦¬ì˜ ëª¨ë“  ì›Œì»¤ë“¤ë„ ì •ë¦¬
        for worker in self.worker_history:
            if worker.isRunning():
                try:
                    if hasattr(worker, 'stop'):
                        worker.stop()
                    worker.wait(2000)
                except:
                    pass
        
        self.worker_history.clear()
        self.current_worker = None
        
        logger.info("ëª¨ë“  ì›Œì»¤ ì •ë¦¬ ì™„ë£Œ")
    
    def is_working(self) -> bool:
        """í˜„ì¬ ì‘ì—… ì¤‘ì¸ì§€ í™•ì¸"""
        return self.current_worker is not None and self.current_worker.isRunning()


class AIAnalysisWorker(QThread):
    """3ë‹¨ê³„: AI í‚¤ì›Œë“œ ë¶„ì„ ì›Œì»¤"""
    
    # ì‹œê·¸ë„ ì •ì˜
    progress_updated = Signal(int, str)  # progress%, message
    analysis_completed = Signal(list)    # List[KeywordBasicData] - AI ë¶„ì„ ê²°ê³¼
    analysis_data_updated = Signal(dict) # ì‹¤ì‹œê°„ ë¶„ì„ ë°ì´í„° ì—…ë°ì´íŠ¸
    error_occurred = Signal(str)         # error_message
    
    def __init__(self, product_names: List[str], prompt: str, selected_keywords: List[str] = None):
        super().__init__()
        self.product_names = product_names
        self.prompt = prompt
        self.selected_keywords = selected_keywords or []  # 1ë‹¨ê³„ì—ì„œ ì„ íƒí•œ í‚¤ì›Œë“œë“¤
        self._stop_requested = False
        self._mutex = QMutex()
    
    def request_stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­"""
        with QMutexLocker(self._mutex):
            self._stop_requested = True
            
    def stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ (í•˜ìœ„ í˜¸í™˜)"""
        self.request_stop()
    
    def is_stopped(self) -> bool:
        """ì¤‘ë‹¨ ìš”ì²­ í™•ì¸"""
        with QMutexLocker(self._mutex):
            return self._stop_requested
    
    def run(self):
        """ì›Œì»¤ ì‹¤í–‰"""
        try:
            logger.info(f"AI ë¶„ì„ ì‹œì‘")
            
            # 1ë‹¨ê³„: AI API í˜¸ì¶œ
            self.progress_updated.emit(10, "AI ëª¨ë¸ì— ë¶„ì„ ìš”ì²­ ì¤‘...")
            
            if self.is_stopped():
                return
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒí’ˆëª… + ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê²°í•©)
            from .engine_local import build_ai_prompt
            
            # ìƒí’ˆëª…ì—ì„œ title ì¶”ì¶œ
            product_titles = []
            for product in self.product_names:
                if isinstance(product, dict):
                    product_titles.append(product.get('title', ''))
                elif isinstance(product, str):
                    product_titles.append(product)
            
            final_prompt = build_ai_prompt(product_titles, self.prompt)
            
            # ì„¤ì •ëœ AI API í˜¸ì¶œ
            ai_response = self.call_ai_api(final_prompt)
            
            # AI ì‘ë‹µ ë°ì´í„° ì—…ë°ì´íŠ¸
            self.analysis_data_updated.emit({
                'input_prompt': final_prompt,
                'ai_response': ai_response
            })
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(50, "AI ì‘ë‹µ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            
            # 2ë‹¨ê³„: AI ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            from .engine_local import parse_ai_keywords_response, deduplicate_keywords
            
            logger.info(f"ğŸ¤– AI ì‘ë‹µ ê¸¸ì´: {len(ai_response)} ë¬¸ì")
            logger.info(f"ğŸ¤– AI ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {ai_response[:200]}...")
            
            extracted_keywords = parse_ai_keywords_response(ai_response)
            
            logger.info(f"ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: {len(extracted_keywords)}")
            logger.info(f"ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°: {extracted_keywords[:10]}")
            
            if not extracted_keywords:
                self.error_occurred.emit("AIì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return
            
            self.progress_updated.emit(60, f"í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° 1ë‹¨ê³„ í‚¤ì›Œë“œ ë³‘í•© ì¤‘... ({len(extracted_keywords)}ê°œ)")
            
            # 3ë‹¨ê³„: AI ì¶”ì¶œ í‚¤ì›Œë“œì™€ 1ë‹¨ê³„ ì„ íƒ í‚¤ì›Œë“œ ë³‘í•©
            all_keywords = extracted_keywords.copy()  # AI ì¶”ì¶œ í‚¤ì›Œë“œ
            
            # 1ë‹¨ê³„ì—ì„œ ì„ íƒí•œ í‚¤ì›Œë“œ ì¶”ê°€ (ì¤‘ë³µ í™•ì¸)
            if self.selected_keywords:
                logger.info(f"ğŸ“‹ 1ë‹¨ê³„ ì„ íƒ í‚¤ì›Œë“œ {len(self.selected_keywords)}ê°œë¥¼ ë³‘í•©í•©ë‹ˆë‹¤: {self.selected_keywords}")
                all_keywords.extend(self.selected_keywords)
            
            # ì¤‘ë³µ ì œê±° ("ê°•ì•„ì§€ê°„ì‹" = "ê°•ì•„ì§€ ê°„ì‹")
            unique_keywords = deduplicate_keywords(all_keywords)
            
            # AI í‚¤ì›Œë“œì™€ 1ë‹¨ê³„ í‚¤ì›Œë“œ ë³‘í•© ê²°ê³¼ ë¡œê·¸
            ai_count = len(extracted_keywords)
            selected_count = len(self.selected_keywords) if self.selected_keywords else 0
            merged_count = len(unique_keywords)
            removed_duplicates = len(all_keywords) - merged_count
            
            logger.info(f"ğŸ”€ í‚¤ì›Œë“œ ë³‘í•© ì™„ë£Œ: AI {ai_count}ê°œ + 1ë‹¨ê³„ {selected_count}ê°œ = ì´ {merged_count}ê°œ (ì¤‘ë³µ ì œê±°: {removed_duplicates}ê°œ)")
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(70, f"{len(unique_keywords)}ê°œ í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì¤‘... (AI {ai_count}ê°œ + ì„ íƒ {selected_count}ê°œ)")
            
            # 4ë‹¨ê³„: ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ (ê¸°ì¡´ keyword_analysis ì„œë¹„ìŠ¤ ì‚¬ìš©)
            logger.info(f"ğŸ“Š ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒí•  í‚¤ì›Œë“œë“¤: {unique_keywords[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from src.features.keyword_analysis.service import KeywordAnalysisService
            from src.features.keyword_analysis.models import AnalysisPolicy, AnalysisScope
            from .models import KeywordBasicData
            
            # ì •ì±… ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ì „ì²´ ë¶„ì„)
            policy = AnalysisPolicy(scope=AnalysisScope.FULL_ANALYSIS)
            analyzed_keywords = []
            
            logger.info(f"ì •ì±… ì„¤ì •: ê²½ìŸë¶„ì„={policy.should_analyze_competition()}, ì¹´í…Œê³ ë¦¬ë¶„ì„={policy.should_analyze_category()}")
            
            # í‚¤ì›Œë“œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° (3ê°œì”© ë³‘ë ¬ ì²˜ë¦¬)
            def analyze_keyword_batch(keywords_batch, start_index):
                """í‚¤ì›Œë“œ ë°°ì¹˜ ë¶„ì„ (ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… í¬í•¨)"""
                from src.foundation.http_client import rate_limiter_manager
                
                analysis_service = KeywordAnalysisService(policy)  # ìŠ¤ë ˆë“œë³„ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
                results = []
                
                # ë„¤ì´ë²„ API ì „ìš© ë ˆì´íŠ¸ ë¦¬ë¯¸í„° (ì´ˆë‹¹ 2íšŒ í˜¸ì¶œ)
                rate_limiter = rate_limiter_manager.get_limiter("naver_searchad", calls_per_second=2.0)
                
                for i, keyword in enumerate(keywords_batch):
                    if self.is_stopped():
                        break
                    
                    # ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ì ìš©
                    with rate_limiter:
                        try:
                            logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘: '{keyword}' ({start_index + i + 1}/{len(unique_keywords)})")
                            kw_data = analysis_service.analyze_single_keyword(keyword)
                            
                            # KeywordDataë¥¼ KeywordBasicDataë¡œ ë³€í™˜ (ì¹´í…Œê³ ë¦¬ëŠ” ì²« ë²ˆì§¸ ì¤„ë§Œ ì‚¬ìš©)
                            category = kw_data.category.split('\n')[0] if kw_data.category else ""
                            basic_data = KeywordBasicData(
                                keyword=kw_data.keyword,
                                search_volume=kw_data.search_volume,
                                category=category
                            )
                            results.append(basic_data)
                            
                            if kw_data.search_volume and kw_data.search_volume > 0:
                                logger.info(f"âœ… '{keyword}' ë¶„ì„ ì™„ë£Œ: ì›”ê²€ìƒ‰ëŸ‰ {kw_data.search_volume}")
                            else:
                                logger.debug(f"âŒ '{keyword}' ë¶„ì„ ì™„ë£Œ: ì›”ê²€ìƒ‰ëŸ‰ 0")
                                
                        except Exception as e:
                            logger.error(f"âŒ í‚¤ì›Œë“œ '{keyword}' ë¶„ì„ ì‹¤íŒ¨: {e}")
                            # ì‹¤íŒ¨í•œ í‚¤ì›Œë“œë„ í¬í•¨ (ê²€ìƒ‰ëŸ‰ 0)
                            results.append(KeywordBasicData(
                                keyword=keyword,
                                search_volume=0,
                                category="ë¶„ì„ ì‹¤íŒ¨"
                            ))
                
                return results
            
            # í‚¤ì›Œë“œë¥¼ 3ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
            batch_size = 3
            total_keywords = len(unique_keywords)
            batches = []
            
            for i in range(0, total_keywords, batch_size):
                batch = unique_keywords[i:i + batch_size]
                batches.append((batch, i))
            
            logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(batches)}ê°œ ë°°ì¹˜, ë°°ì¹˜ë‹¹ {batch_size}ê°œ í‚¤ì›Œë“œ")
            self.progress_updated.emit(25, f"{total_keywords}ê°œ í‚¤ì›Œë“œë¥¼ 3ê°œì”© ë°°ì¹˜ë¡œ ë³‘ë ¬ ë¶„ì„ ì‹œì‘...")
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 3ê°œ ìŠ¤ë ˆë“œ)
            with ThreadPoolExecutor(max_workers=3) as executor:
                # ëª¨ë“  ë°°ì¹˜ë¥¼ ì œì¶œ
                future_to_batch = {
                    executor.submit(analyze_keyword_batch, batch, start_idx): (batch, start_idx)
                    for batch, start_idx in batches
                }
                
                completed_count = 0
                for future in as_completed(future_to_batch):
                    if self.is_stopped():
                        # ëª¨ë“  ì‘ì—… ì·¨ì†Œ
                        for f in future_to_batch:
                            f.cancel()
                        break
                    
                    batch, start_idx = future_to_batch[future]
                    
                    try:
                        batch_results = future.result()
                        analyzed_keywords.extend(batch_results)
                        
                        completed_count += len(batch)
                        progress = int((completed_count / total_keywords) * 85)
                        
                        # í˜„ì¬ ë°°ì¹˜ì˜ í‚¤ì›Œë“œë“¤ì„ ì§„í–‰ë¥  ë©”ì‹œì§€ì— í¬í•¨
                        batch_keywords = [kw for kw in batch]  # í˜„ì¬ ì²˜ë¦¬ëœ ë°°ì¹˜ì˜ í‚¤ì›Œë“œë“¤
                        if len(batch_keywords) == 1:
                            current_keyword = batch_keywords[0]
                        else:
                            current_keyword = f"{batch_keywords[0]} ì™¸ {len(batch_keywords)-1}ê°œ"
                        
                        self.progress_updated.emit(
                            progress, 
                            f"'{current_keyword}' ë¶„ì„ ì™„ë£Œ ({completed_count}/{total_keywords})"
                        )
                        
                        logger.info(f"ë°°ì¹˜ ì™„ë£Œ: {len(batch_results)}ê°œ í‚¤ì›Œë“œ ë¶„ì„ë¨ - {batch_keywords}")
                        
                    except Exception as e:
                        logger.error(f"ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
                        # ì‹¤íŒ¨í•œ ë°°ì¹˜ì˜ í‚¤ì›Œë“œë“¤ì„ 0 ê²€ìƒ‰ëŸ‰ìœ¼ë¡œ ì¶”ê°€
                        for keyword in batch:
                            analyzed_keywords.append(KeywordBasicData(
                                keyword=keyword,
                                search_volume=0,
                                category="ë¶„ì„ ì‹¤íŒ¨"
                            ))
            
            # ê²°ê³¼ ì •ë ¬ (ì›ë˜ ìˆœì„œ ìœ ì§€)
            keyword_order = {kw: i for i, kw in enumerate(unique_keywords)}
            analyzed_keywords.sort(key=lambda x: keyword_order.get(x.keyword, 999999))
            
            logger.info(f"ğŸ“Š ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì™„ë£Œ: {len(analyzed_keywords)}ê°œ ì¤‘ ê²€ìƒ‰ëŸ‰ ìˆëŠ” í‚¤ì›Œë“œ {len([kw for kw in analyzed_keywords if kw.search_volume > 0])}ê°œ")
            
            # ë¶„ì„ ë°ì´í„° ì—…ë°ì´íŠ¸ (ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ê²°ê³¼)
            self.analysis_data_updated.emit({
                'analyzed_keywords': analyzed_keywords,
                'extracted_keywords': unique_keywords
            })
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(85, "ì›”ê²€ìƒ‰ëŸ‰ 100 ì´ìƒ í‚¤ì›Œë“œ í•„í„°ë§ ì¤‘...")
            
            # 5ë‹¨ê³„: ì›”ê²€ìƒ‰ëŸ‰ 100 ì´ìƒ í•„í„°ë§
            from .engine_local import filter_keywords_by_search_volume
            volume_filtered = filter_keywords_by_search_volume(analyzed_keywords, 100)
            
            if not volume_filtered:
                self.error_occurred.emit("ì›”ê²€ìƒ‰ëŸ‰ 100 ì´ìƒì¸ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            self.progress_updated.emit(95, f"{len(volume_filtered)}ê°œ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì¤‘...")
            
            # 6ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ì •ë³´ ë³´ê°• (100 ì´ìƒ í‚¤ì›Œë“œë§Œ)
            # TODO: ì—¬ê¸°ì— ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ë¡œì§ ì¶”ê°€ ì˜ˆì •
            filtered_keywords = volume_filtered
            
            # ìµœì¢… ë¶„ì„ ë°ì´í„° ì—…ë°ì´íŠ¸
            self.analysis_data_updated.emit({
                'filtered_keywords': filtered_keywords
            })
            
            self.progress_updated.emit(100, f"AI ë¶„ì„ ì™„ë£Œ: {len(filtered_keywords)}ê°œ í‚¤ì›Œë“œ")
            
            # ì™„ë£Œ ì‹œê·¸ë„ ë°œì†¡
            self.analysis_completed.emit(filtered_keywords)
            
            logger.info(f"AI ë¶„ì„ ì™„ë£Œ: {len(filtered_keywords)}ê°œ í‚¤ì›Œë“œ")
            
        except Exception as e:
            logger.error(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.error_occurred.emit(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    def call_ai_api(self, prompt: str) -> str:
        """ì‚¬ìš©ìê°€ ì„¤ì •í•œ AI API í˜¸ì¶œ"""
        try:
            from src.foundation.config import config_manager
            api_config = config_manager.load_api_config()
            
            # í˜„ì¬ ì„ íƒëœ AI ëª¨ë¸ í™•ì¸
            current_model = getattr(api_config, 'current_ai_model', '')
            if not current_model or current_model == "AI ì œê³µìë¥¼ ì„ íƒí•˜ì„¸ìš”":
                raise Exception("AI ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì • ë©”ë‰´ì—ì„œ AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ API í˜¸ì¶œ
            if "GPT" in current_model:
                if not hasattr(api_config, 'openai_api_key') or not api_config.openai_api_key:
                    raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"{current_model}ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
                return self.call_openai_direct(prompt, api_config.openai_api_key, current_model)
                
            elif "Gemini" in current_model:
                if not hasattr(api_config, 'gemini_api_key') or not api_config.gemini_api_key:
                    raise Exception("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"{current_model}ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
                return self.call_gemini_direct(prompt, api_config.gemini_api_key, current_model)
                
            elif "Claude" in current_model:
                if not hasattr(api_config, 'claude_api_key') or not api_config.claude_api_key:
                    raise Exception("Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"{current_model}ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
                return self.call_claude_direct(prompt, api_config.claude_api_key, current_model)
            else:
                raise Exception(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” AI ëª¨ë¸ì…ë‹ˆë‹¤: {current_model}")
                
        except Exception as e:
            logger.error(f"AI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def call_openai_direct(self, prompt: str, api_key: str, model_name: str) -> str:
        """OpenAI API ì§ì ‘ í˜¸ì¶œ"""
        import requests
        
        try:
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            
            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì‹¤ì œ ëª¨ë¸ ID ì„¤ì •
            if "GPT-4o Mini" in model_name:
                model_id = "gpt-4o-mini"
                max_tokens = 16384
            elif "GPT-4o" in model_name and "Mini" not in model_name:
                model_id = "gpt-4o"
                max_tokens = 8192
            elif "GPT-4 Turbo" in model_name:
                model_id = "gpt-4-turbo"
                max_tokens = 8192
            else:
                model_id = "gpt-4o-mini"  # ê¸°ë³¸ê°’
                max_tokens = 16384
            
            payload = {
                "model": model_id,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.3
            }
            
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'choices' in data and len(data['choices']) > 0:
                    return data['choices'][0]['message']['content']
                else:
                    raise Exception("OpenAI API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise Exception(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def call_gemini_direct(self, prompt: str, api_key: str, model_name: str) -> str:
        """Gemini API ì§ì ‘ í˜¸ì¶œ"""
        import requests
        
        try:
            headers = {
                'Content-Type': 'application/json'
            }
            
            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì‹¤ì œ ëª¨ë¸ ID ì„¤ì •
            if "Gemini 1.5 Flash" in model_name:
                model_id = "gemini-1.5-flash-latest"
                max_tokens = 8192
            elif "Gemini 1.5 Pro" in model_name:
                model_id = "gemini-1.5-pro-latest"
                max_tokens = 8192
            elif "Gemini 2.0 Flash" in model_name:
                model_id = "gemini-2.0-flash-exp"
                max_tokens = 8192
            else:
                model_id = "gemini-1.5-flash-latest"  # ê¸°ë³¸ê°’
                max_tokens = 8192
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "temperature": 0.3,
                    "maxOutputTokens": max_tokens
                }
            }
            
            url = f'https://generativelanguage.googleapis.com/v1beta/models/{model_id}:generateContent?key={api_key}'
            
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'candidates' in data and len(data['candidates']) > 0:
                    content = data['candidates'][0].get('content', {})
                    parts = content.get('parts', [])
                    if parts:
                        return parts[0].get('text', '')
                    else:
                        raise Exception("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                else:
                    raise Exception("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"Gemini API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def call_claude_direct(self, prompt: str, api_key: str, model_name: str) -> str:
        """Claude API ì§ì ‘ í˜¸ì¶œ"""
        import requests
        
        try:
            headers = {
                'x-api-key': api_key,
                'Content-Type': 'application/json',
                'anthropic-version': '2023-06-01'
            }
            
            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì‹¤ì œ ëª¨ë¸ ID ì„¤ì •
            if "Claude 3.5 Sonnet" in model_name:
                model_id = "claude-3-5-sonnet-20241022"
                max_tokens = 8192
            elif "Claude 3.5 Haiku" in model_name:
                model_id = "claude-3-5-haiku-20241022"
                max_tokens = 8192
            elif "Claude 3 Opus" in model_name:
                model_id = "claude-3-opus-20240229"
                max_tokens = 8192
            else:
                model_id = "claude-3-5-sonnet-20241022"  # ê¸°ë³¸ê°’
                max_tokens = 8192
            
            payload = {
                "model": model_id,
                "max_tokens": max_tokens,
                "temperature": 0.3,
                "messages": [{"role": "user", "content": prompt}]
            }
            
            response = requests.post(
                'https://api.anthropic.com/v1/messages',
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'content' in data and len(data['content']) > 0:
                    return data['content'][0]['text']
                else:
                    raise Exception("Claude API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"Claude API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise Exception(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì›Œì»¤ ë§¤ë‹ˆì €
worker_manager = WorkerManager()