"""
ë„¤ì´ë²„ ìƒí’ˆëª… ìƒì„±ê¸° ì–´ëŒ‘í„°
ë²¤ë” API í˜¸ì¶œ ë° ë°ì´í„° ì •ê·œí™”, ì—‘ì…€ ì €ì¥ ë‹´ë‹¹ (keyword_analysis ìŠ¤íƒ€ì¼)
"""
from typing import List, Dict, Any, Optional, Callable
import re
from collections import Counter

from src.foundation.logging import get_logger
from src.foundation.http_client import api_error_handler, ParallelAPIProcessor, rate_limiter_manager
from .models import KeywordBasicData

logger = get_logger("features.naver_product_title_generator.adapters")


def parse_keywords(text: str) -> List[str]:
    """
    ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì—”í„° ë˜ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„)
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        
    Returns:
        List[str]: ì •ë¦¬ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    if not text or not text.strip():
        return []
    
    # ì‰¼í‘œì™€ ì—”í„°ë¡œ êµ¬ë¶„
    keywords = re.split(r'[,\n]+', text.strip())
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°í•˜ê³  ê³µë°± ì •ë¦¬
    cleaned_keywords = []
    for keyword in keywords:
        cleaned = keyword.strip()
        if cleaned:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
            cleaned_keywords.append(cleaned)
    
    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
    unique_keywords = []
    seen = set()
    for keyword in cleaned_keywords:
        keyword_lower = keyword.lower()
        if keyword_lower not in seen:
            seen.add(keyword_lower)
            unique_keywords.append(keyword)
    
    logger.debug(f"í‚¤ì›Œë“œ íŒŒì‹± ì™„ë£Œ: {len(unique_keywords)}ê°œ í‚¤ì›Œë“œ")
    return unique_keywords


@api_error_handler("ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  API")
def get_keyword_search_volume(keyword: str) -> int:
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  APIë¡œ í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ (Raw ë°©ì‹)
    
    Args:
        keyword: ì¡°íšŒí•  í‚¤ì›Œë“œ
        
    Returns:
        int: ì›”ê²€ìƒ‰ëŸ‰ (PC + ëª¨ë°”ì¼ í•©ê³„), ì¡°íšŒ ì‹¤íŒ¨ì‹œ 0
    """
    try:
        from src.vendors.naver.client_factory import NaverClientFactory
        from .engine_local import normalize_keyword_for_api
        
        # API í˜¸ì¶œìš© í‚¤ì›Œë“œ ì •ê·œí™” (ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        normalized_keyword = normalize_keyword_for_api(keyword)
        
        logger.debug(f"ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì‹œì‘: '{keyword}' â†’ '{normalized_keyword}'")
        
        # ê²€ìƒ‰ê´‘ê³  API í´ë¼ì´ì–¸íŠ¸
        keyword_client = NaverClientFactory.get_keyword_tool_client()
        searchad_response = keyword_client.get_keyword_ideas([normalized_keyword])
        
        if not searchad_response or 'keywordList' not in searchad_response:
            logger.warning(f"ê²€ìƒ‰ê´‘ê³  API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ: '{keyword}'")
            return 0
        
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸° (ì •ê·œí™”ëœ í‚¤ì›Œë“œë¡œ ë¹„êµ)
        for kw_item in searchad_response['keywordList']:
            if kw_item.get('relKeyword', '').upper() == normalized_keyword.upper():
                pc_count = kw_item.get('monthlyPcQcCnt', 0)
                mobile_count = kw_item.get('monthlyMobileQcCnt', 0)
                
                # "< 10" ì²˜ë¦¬
                if pc_count == '< 10':
                    pc_count = 0
                elif isinstance(pc_count, str):
                    pc_count = int(pc_count) if pc_count.isdigit() else 0
                
                if mobile_count == '< 10':
                    mobile_count = 0
                elif isinstance(mobile_count, str):
                    mobile_count = int(mobile_count) if mobile_count.isdigit() else 0
                
                total_volume = int(pc_count) + int(mobile_count)
                logger.debug(f"ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì™„ë£Œ: '{keyword}' -> {total_volume} (PC: {pc_count}, Mobile: {mobile_count})")
                return total_volume
        
        logger.warning(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ê²€ìƒ‰ëŸ‰ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return 0
        
    except Exception as e:
        logger.error(f"ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨ '{keyword}': {e}")
        return 0


@api_error_handler("ë„¤ì´ë²„ ì‡¼í•‘ API")
def get_keyword_category(keyword: str) -> str:
    """
    ë„¤ì´ë²„ ê°œë°œì ì‡¼í•‘ APIë¡œ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ (Raw ë°©ì‹)
    
    Args:
        keyword: ì¡°íšŒí•  í‚¤ì›Œë“œ
        
    Returns:
        str: ì¹´í…Œê³ ë¦¬ ê²½ë¡œ (ì˜ˆ: "íŒ¨ì…˜ì˜ë¥˜ > ì—¬ì„±ì˜ë¥˜ > ì›í”¼ìŠ¤"), ì¡°íšŒ ì‹¤íŒ¨ì‹œ ë¹ˆ ë¬¸ìì—´
    """
    try:
        from src.vendors.naver.client_factory import NaverClientFactory
        from .engine_local import normalize_keyword_for_api
        
        # API í˜¸ì¶œìš© í‚¤ì›Œë“œ ì •ê·œí™” (ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        normalized_keyword = normalize_keyword_for_api(keyword)
        
        logger.debug(f"ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œì‘: '{keyword}' â†’ '{normalized_keyword}'")
        
        # ì‡¼í•‘ API í´ë¼ì´ì–¸íŠ¸
        shopping_client = NaverClientFactory.get_shopping_client()
        shopping_response = shopping_client.search_products(query=normalized_keyword, display=20, sort="sim")
        
        # ì‘ë‹µ êµ¬ì¡° ë¡œê·¸ (ë””ë²„ê¹…ìš©)
        logger.debug(f"ì‡¼í•‘ API ì‘ë‹µ êµ¬ì¡° í™•ì¸: '{keyword}' -> {type(shopping_response)}")
        
        # NaverShoppingResponse ê°ì²´ ì²˜ë¦¬
        if hasattr(shopping_response, 'items'):
            items = shopping_response.items
            if not items:
                logger.warning(f"ì‡¼í•‘ API ì‘ë‹µì— itemsê°€ ë¹„ì–´ìˆìŒ: '{keyword}'")
                return ""
        # ë”•ì…”ë„ˆë¦¬ ì‘ë‹µ ì²˜ë¦¬ (í˜¹ì‹œ raw ì‘ë‹µì¸ ê²½ìš°)
        elif isinstance(shopping_response, dict) and 'items' in shopping_response:
            items = shopping_response['items']
            if not items:
                logger.warning(f"ì‡¼í•‘ API ì‘ë‹µì— itemsê°€ ë¹„ì–´ìˆìŒ: '{keyword}'")
                return ""
        else:
            logger.warning(f"ì‡¼í•‘ API ì‘ë‹µ í˜•íƒœë¥¼ ì¸ì‹í•  ìˆ˜ ì—†ìŒ: '{keyword}' -> {type(shopping_response)}")
            return ""
        
        logger.debug(f"ì‡¼í•‘ API ì‘ë‹µ: '{keyword}' -> {len(items)}ê°œ ìƒí’ˆ ë°œê²¬")
        
        # ëª¨ë“  ìƒí’ˆ(1~40ìœ„)ì˜ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
        all_categories = []
        
        for idx, item in enumerate(items):
            try:
                category_path = None
                
                # NaverShoppingItem ê°ì²´ì¸ ê²½ìš°
                if hasattr(item, 'category1'):
                    categories = []
                    for i in range(1, 10):  # category1~9ê¹Œì§€ í™•ì¸
                        cat_attr = f'category{i}'
                        if hasattr(item, cat_attr):
                            cat_value = getattr(item, cat_attr)
                            if cat_value and cat_value.strip():  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                                categories.append(cat_value.strip())
                    
                    if categories:
                        category_path = " > ".join(categories)
                
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                elif isinstance(item, dict):
                    categories = []
                    for i in range(1, 10):  # category1~9ê¹Œì§€ í™•ì¸
                        cat_key = f'category{i}'
                        if cat_key in item and item[cat_key] and item[cat_key].strip():
                            categories.append(item[cat_key].strip())
                    
                    if categories:
                        category_path = " > ".join(categories)
                    
                    # ë””ë²„ê¹…ìš©: ì²« ë²ˆì§¸ ìƒí’ˆì˜ ëª¨ë“  í•„ë“œ ì¶œë ¥
                    if idx == 0:
                        logger.debug(f"ì²« ë²ˆì§¸ ìƒí’ˆ í•„ë“œë“¤: {list(item.keys())}")
                        category_fields = [k for k in item.keys() if 'category' in k.lower()]
                        logger.debug(f"ì¹´í…Œê³ ë¦¬ ê´€ë ¨ í•„ë“œë“¤: {category_fields}")
                        for field in category_fields:
                            logger.debug(f"{field}: {item[field]}")
                
                # ì¹´í…Œê³ ë¦¬ ê²½ë¡œê°€ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                if category_path:
                    all_categories.append(category_path)
                    logger.debug(f"ìƒí’ˆ{idx+1}: {category_path}")
                
            except Exception as e:
                logger.warning(f"ìƒí’ˆ {idx+1}ì˜ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¹´í…Œê³ ë¦¬ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ê²½ìš°
        if not all_categories:
            logger.warning(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. items ê°œìˆ˜: {len(items)}")
            return ""
        
        # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
        from collections import Counter
        category_counter = Counter(all_categories)
        most_common_category, count = category_counter.most_common(1)[0]
        
        # í¼ì„¼í…Œì´ì§€ ê³„ì‚°
        total_products = len(all_categories)
        percentage = int((count / total_products) * 100)
        
        # ê²°ê³¼ í¬ë§·: "ì¹´í…Œê³ ë¦¬ ê²½ë¡œ (í¼ì„¼í…Œì´ì§€%)"
        result = f"{most_common_category} ({percentage}%)"
        
        logger.info(f"ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì™„ë£Œ: '{keyword}' -> '{result}' ({count}/{total_products}ê°œ ìƒí’ˆ)")
        return result
        
    except Exception as e:
        logger.error(f"ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ '{keyword}': {e}")
        return "ì¡°íšŒ ì‹¤íŒ¨"


def analyze_keyword_full_info(keyword: str) -> KeywordBasicData:
    """
    í‚¤ì›Œë“œì˜ ì›”ê²€ìƒ‰ëŸ‰ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ëª¨ë‘ ì¡°íšŒí•´ì„œ KeywordBasicData ë°˜í™˜
    
    Args:
        keyword: ë¶„ì„í•  í‚¤ì›Œë“œ
        
    Returns:
        KeywordBasicData: í‚¤ì›Œë“œ, ì›”ê²€ìƒ‰ëŸ‰, ì¹´í…Œê³ ë¦¬ ì •ë³´
    """
    try:
        logger.info(f"í‚¤ì›Œë“œ ì „ì²´ ë¶„ì„ ì‹œì‘: '{keyword}'")
        
        # 1. ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ
        search_volume = get_keyword_search_volume(keyword)
        
        # 2. ì¹´í…Œê³ ë¦¬ ì¡°íšŒ  
        category = get_keyword_category(keyword)
        
        # KeywordBasicData ìƒì„±
        keyword_data = KeywordBasicData(
            keyword=keyword,
            search_volume=search_volume,
            category=category or "ì¹´í…Œê³ ë¦¬ ì—†ìŒ"
        )
        
        logger.info(f"í‚¤ì›Œë“œ ì „ì²´ ë¶„ì„ ì™„ë£Œ: '{keyword}' (ê²€ìƒ‰ëŸ‰: {search_volume}, ì¹´í…Œê³ ë¦¬: {category or 'ì—†ìŒ'})")
        return keyword_data
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì „ì²´ ë¶„ì„ ì‹¤íŒ¨ '{keyword}': {e}")
        return KeywordBasicData(
            keyword=keyword,
            search_volume=0,
            category="ë¶„ì„ ì‹¤íŒ¨"
        )


def get_keywords_search_volume(keywords: List[str], 
                              max_workers: int = 3,
                              stop_check: Optional[Callable[[], bool]] = None,
                              progress_callback: Optional[Callable[[int, int, str], None]] = None) -> List[KeywordBasicData]:
    """
    í‚¤ì›Œë“œë“¤ì˜ ì›”ê²€ìƒ‰ëŸ‰ì„ ë³‘ë ¬ë¡œ ì¡°íšŒ (adapters ì—­í• : ë²¤ë” í˜¸ì¶œ + ì •ê·œí™”)
    
    Args:
        keywords: ë¶„ì„í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        max_workers: ìµœëŒ€ ë™ì‹œ ì‘ì—… ìˆ˜ (ê¸°ë³¸ 3ê°œ)
        stop_check: ì¤‘ë‹¨ í™•ì¸ í•¨ìˆ˜
        progress_callback: ì§„í–‰ë¥  ì½œë°±
        
    Returns:
        List[KeywordBasicData]: ì›”ê²€ìƒ‰ëŸ‰ë§Œ í¬í•¨ëœ ì •ê·œí™”ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if not keywords:
        return []
    
    # ë‹¨ì¼ í‚¤ì›Œë“œì¸ ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬ ì—†ì´ ì§ì ‘ í˜¸ì¶œ
    if len(keywords) == 1:
        try:
            search_volume = get_keyword_search_volume(keywords[0])
            return [KeywordBasicData(keyword=keywords[0], search_volume=search_volume, category="")]
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨ '{keywords[0]}': {e}")
            return [KeywordBasicData(keyword=keywords[0], search_volume=0, category="")]
    
    # ì—¬ëŸ¬ í‚¤ì›Œë“œëŠ” ë³‘ë ¬ ì²˜ë¦¬
    try:
        logger.info(f"ğŸ”„ ë³‘ë ¬ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì‹œì‘: {len(keywords)}ê°œ í‚¤ì›Œë“œ, {max_workers}ê°œ ì›Œì»¤")
        
        # ë ˆì´íŠ¸ ë¦¬ë¯¸í„° ì„¤ì •
        api_limiter = rate_limiter_manager.get_limiter("naver_volume_api", calls_per_second=1.0)
        
        # ë³‘ë ¬ ì²˜ë¦¬ê¸° ìƒì„±
        processor = ParallelAPIProcessor(max_workers=max_workers, rate_limiter=api_limiter)
        
        # ë‹¨ì¼ í‚¤ì›Œë“œ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
        def process_single_keyword(keyword: str) -> KeywordBasicData:
            try:
                search_volume = get_keyword_search_volume(keyword)
                return KeywordBasicData(keyword=keyword, search_volume=search_volume, category="")
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨ '{keyword}': {e}")
                return KeywordBasicData(keyword=keyword, search_volume=0, category="")
        
        # ì§„í–‰ë¥  ì½œë°± ë˜í¼ (í‚¤ì›Œë“œëª… í¬í•¨)
        def detailed_progress_callback(current: int, total: int, message: str):
            if progress_callback:
                # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í‚¤ì›Œë“œ í‘œì‹œ
                if current <= len(keywords):
                    current_keyword = keywords[current-1] if current > 0 else ""
                    detailed_message = f"ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì¤‘... ({current}/{total}) '{current_keyword}'"
                    progress_callback(current, total, detailed_message)
                else:
                    progress_callback(current, total, message)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_batch(
            func=process_single_keyword,
            items=keywords,
            stop_check=stop_check,
            progress_callback=detailed_progress_callback
        )
        
        # ê²°ê³¼ ì •ë¦¬
        keyword_data_list = []
        failed_count = 0
        
        for keyword, result, error in results:
            if error is None and result is not None:
                keyword_data_list.append(result)
            else:
                failed_count += 1
                keyword_data_list.append(KeywordBasicData(keyword=keyword, search_volume=0, category=""))
        
        if failed_count > 0:
            logger.warning(f"âš ï¸ {failed_count}ê°œ í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨")
        
        logger.info(f"âœ… ë³‘ë ¬ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì™„ë£Œ: ì„±ê³µ {len(keyword_data_list)-failed_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ")
        return keyword_data_list
        
    except Exception as e:
        logger.error(f"ë³‘ë ¬ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return [KeywordBasicData(keyword=kw, search_volume=0, category="") for kw in keywords]


def get_keywords_category(keyword_data_list: List[KeywordBasicData], 
                         max_workers: int = 3,
                         stop_check: Optional[Callable[[], bool]] = None,
                         progress_callback: Optional[Callable[[int, int, str], None]] = None) -> List[KeywordBasicData]:
    """
    í‚¤ì›Œë“œë“¤ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì¡°íšŒí•˜ì—¬ ì—…ë°ì´íŠ¸ (adapters ì—­í• : ë²¤ë” í˜¸ì¶œ + ì •ê·œí™”)
    
    Args:
        keyword_data_list: ì¹´í…Œê³ ë¦¬ë¥¼ ì¡°íšŒí•  í‚¤ì›Œë“œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        max_workers: ìµœëŒ€ ë™ì‹œ ì‘ì—… ìˆ˜ (ê¸°ë³¸ 3ê°œ)
        stop_check: ì¤‘ë‹¨ í™•ì¸ í•¨ìˆ˜
        progress_callback: ì§„í–‰ë¥  ì½œë°±
        
    Returns:
        List[KeywordBasicData]: ì¹´í…Œê³ ë¦¬ê°€ ì—…ë°ì´íŠ¸ëœ ì •ê·œí™”ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if not keyword_data_list:
        return []
    
    # ë‹¨ì¼ í‚¤ì›Œë“œì¸ ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬ ì—†ì´ ì§ì ‘ í˜¸ì¶œ
    if len(keyword_data_list) == 1:
        kd = keyword_data_list[0]
        try:
            category = get_keyword_category(kd.keyword)
            return [KeywordBasicData(keyword=kd.keyword, search_volume=kd.search_volume, category=category or "ì¹´í…Œê³ ë¦¬ ì—†ìŒ")]
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ '{kd.keyword}': {e}")
            return [KeywordBasicData(keyword=kd.keyword, search_volume=kd.search_volume, category="ì¡°íšŒ ì‹¤íŒ¨")]
    
    # ì—¬ëŸ¬ í‚¤ì›Œë“œëŠ” ë³‘ë ¬ ì²˜ë¦¬
    try:
        logger.info(f"ğŸ”„ ë³‘ë ¬ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œì‘: {len(keyword_data_list)}ê°œ í‚¤ì›Œë“œ, {max_workers}ê°œ ì›Œì»¤")
        
        # ë ˆì´íŠ¸ ë¦¬ë¯¸í„° ì„¤ì • (ì¹´í…Œê³ ë¦¬ ì¡°íšŒë„ ë™ì¼í•œ ì†ë„)
        api_limiter = rate_limiter_manager.get_limiter("naver_category_api", calls_per_second=1.0)
        
        # ë³‘ë ¬ ì²˜ë¦¬ê¸° ìƒì„±
        processor = ParallelAPIProcessor(max_workers=max_workers, rate_limiter=api_limiter)
        
        # í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        keywords = [kd.keyword for kd in keyword_data_list]
        
        # ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
        def process_single_category(keyword: str) -> str:
            try:
                category = get_keyword_category(keyword)
                return category or "ì¹´í…Œê³ ë¦¬ ì—†ìŒ"
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ '{keyword}': {e}")
                return "ì¡°íšŒ ì‹¤íŒ¨"
        
        # ì§„í–‰ë¥  ì½œë°± ë˜í¼ (í‚¤ì›Œë“œëª… í¬í•¨)
        def detailed_progress_callback(current: int, total: int, message: str):
            if progress_callback:
                # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í‚¤ì›Œë“œ í‘œì‹œ
                if current <= len(keywords):
                    current_keyword = keywords[current-1] if current > 0 else ""
                    detailed_message = f"ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì¤‘... ({current}/{total}) '{current_keyword}'"
                    progress_callback(current, total, detailed_message)
                else:
                    progress_callback(current, total, message)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_batch(
            func=process_single_category,
            items=keywords,
            stop_check=stop_check,
            progress_callback=detailed_progress_callback
        )
        
        # ê²°ê³¼ ë³‘í•© (ì›ë˜ KeywordBasicDataì— ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—…ë°ì´íŠ¸)
        updated_keyword_data = []
        failed_count = 0
        
        for i, (keyword, category_result, error) in enumerate(results):
            original_data = keyword_data_list[i]
            
            if error is None and category_result is not None:
                updated_data = KeywordBasicData(
                    keyword=original_data.keyword,
                    search_volume=original_data.search_volume,
                    category=category_result
                )
            else:
                failed_count += 1
                updated_data = KeywordBasicData(
                    keyword=original_data.keyword,
                    search_volume=original_data.search_volume,
                    category="ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨"
                )
            
            updated_keyword_data.append(updated_data)
        
        if failed_count > 0:
            logger.warning(f"âš ï¸ {failed_count}ê°œ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨")
        
        logger.info(f"âœ… ë³‘ë ¬ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì™„ë£Œ: ì„±ê³µ {len(updated_keyword_data)-failed_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ")
        return updated_keyword_data
        
    except Exception as e:
        logger.error(f"ë³‘ë ¬ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì›ë˜ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì¹´í…Œê³ ë¦¬ë§Œ ì‹¤íŒ¨ í‘œì‹œ)
        return [KeywordBasicData(
            keyword=kd.keyword,
            search_volume=kd.search_volume,
            category="ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨"
        ) for kd in keyword_data_list]


def analyze_keywords_parallel(keywords: List[str], 
                            max_workers: int = 3,
                            stop_check: Optional[Callable[[], bool]] = None,
                            progress_callback: Optional[Callable[[int, int, str], None]] = None) -> List[KeywordBasicData]:
    """
    í‚¤ì›Œë“œë“¤ì„ ë³‘ë ¬ë¡œ ë¶„ì„ (ì›”ê²€ìƒ‰ëŸ‰ + ì¹´í…Œê³ ë¦¬)
    
    Args:
        keywords: ë¶„ì„í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        max_workers: ìµœëŒ€ ë™ì‹œ ì‘ì—… ìˆ˜ (ê¸°ë³¸ 3ê°œ)
        stop_check: ì¤‘ë‹¨ í™•ì¸ í•¨ìˆ˜
        progress_callback: ì§„í–‰ë¥  ì½œë°±
        
    Returns:
        List[KeywordBasicData]: ë¶„ì„ëœ í‚¤ì›Œë“œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    try:
        logger.info(f"ğŸ”„ ë³‘ë ¬ í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘: {len(keywords)}ê°œ í‚¤ì›Œë“œ, {max_workers}ê°œ ì›Œì»¤")
        
        # ë ˆì´íŠ¸ ë¦¬ë¯¸í„° ì„¤ì • (ì´ˆë‹¹ 1íšŒ í˜¸ì¶œë¡œ ì•ˆì „í•˜ê²Œ)
        api_limiter = rate_limiter_manager.get_limiter("naver_parallel_api", calls_per_second=1.0)
        
        # ë³‘ë ¬ ì²˜ë¦¬ê¸° ìƒì„±
        processor = ParallelAPIProcessor(max_workers=max_workers, rate_limiter=api_limiter)
        
        # ì§„í–‰ë¥  ì½œë°± ë˜í¼ (í‚¤ì›Œë“œëª… í¬í•¨)
        def detailed_progress_callback(current: int, total: int, message: str):
            if progress_callback:
                # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í‚¤ì›Œë“œ í‘œì‹œ
                if current <= len(keywords):
                    current_keyword = keywords[current-1] if current > 0 else ""
                    detailed_message = f"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘... ({current}/{total}) '{current_keyword}'"
                    progress_callback(current, total, detailed_message)
                else:
                    progress_callback(current, total, message)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_batch(
            func=analyze_keyword_full_info,
            items=keywords,
            stop_check=stop_check,
            progress_callback=detailed_progress_callback
        )
        
        # ê²°ê³¼ ì •ë¦¬
        keyword_data_list = []
        failed_count = 0
        
        for keyword, result, error in results:
            if error is None and result is not None:
                keyword_data_list.append(result)
            else:
                # ì‹¤íŒ¨í•œ í‚¤ì›Œë“œëŠ” ê¸°ë³¸ ë°ì´í„°ë¡œ ì¶”ê°€
                failed_count += 1
                logger.warning(f"í‚¤ì›Œë“œ '{keyword}' ë¶„ì„ ì‹¤íŒ¨: {error}")
                keyword_data_list.append(KeywordBasicData(
                    keyword=keyword,
                    search_volume=0,
                    category="ë¶„ì„ ì‹¤íŒ¨"
                ))
        
        success_count = len(keyword_data_list) - failed_count
        logger.info(f"âœ… ë³‘ë ¬ í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ: {success_count}/{len(keywords)} ì„±ê³µ")
        
        return keyword_data_list
        
    except Exception as e:
        logger.error(f"ë³‘ë ¬ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        # ì „ì²´ ì‹¤íŒ¨ ì‹œ ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì‹¤íŒ¨ ë°ì´í„°ë¡œ ë°˜í™˜
        return [KeywordBasicData(
            keyword=keyword,
            search_volume=0,
            category="ë¶„ì„ ì‹¤íŒ¨"
        ) for keyword in keywords]


def analyze_keywords_batch(keywords: List[str], 
                         batch_size: int = 10,
                         max_workers: int = 3,
                         stop_check: Optional[Callable[[], bool]] = None,
                         progress_callback: Optional[Callable[[int, int, str], None]] = None) -> List[KeywordBasicData]:
    """
    í‚¤ì›Œë“œë“¤ì„ ë°°ì¹˜ë³„ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ìš©)
    
    Args:
        keywords: ë¶„ì„í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ 10ê°œ)
        max_workers: ë°°ì¹˜ë‹¹ ìµœëŒ€ ë™ì‹œ ì‘ì—… ìˆ˜ (ê¸°ë³¸ 3ê°œ)
        stop_check: ì¤‘ë‹¨ í™•ì¸ í•¨ìˆ˜
        progress_callback: ì§„í–‰ë¥  ì½œë°±
        
    Returns:
        List[KeywordBasicData]: ë¶„ì„ëœ í‚¤ì›Œë“œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    try:
        total_keywords = len(keywords)
        logger.info(f"ğŸ”„ ë°°ì¹˜ë³„ ë³‘ë ¬ ë¶„ì„ ì‹œì‘: {total_keywords}ê°œ í‚¤ì›Œë“œ, ë°°ì¹˜ í¬ê¸° {batch_size}, ì›Œì»¤ {max_workers}ê°œ")
        
        all_results = []
        processed_count = 0
        
        # í‚¤ì›Œë“œë¥¼ ë°°ì¹˜ë³„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, total_keywords, batch_size):
            if stop_check and stop_check():
                break
            
            batch_keywords = keywords[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (total_keywords + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘: {len(batch_keywords)}ê°œ í‚¤ì›Œë“œ")
            
            # ë°°ì¹˜ë³„ ì§„í–‰ë¥  ì½œë°± ë˜í•‘
            def batch_progress_callback(current: int, total: int, message: str):
                global_current = processed_count + current
                if progress_callback:
                    progress_callback(global_current, total_keywords, f"ë°°ì¹˜ {batch_num}/{total_batches}: {message}")
            
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_results = analyze_keywords_parallel(
                keywords=batch_keywords,
                max_workers=max_workers,
                stop_check=stop_check,
                progress_callback=batch_progress_callback
            )
            
            all_results.extend(batch_results)
            processed_count += len(batch_keywords)
        
        logger.info(f"âœ… ë°°ì¹˜ë³„ ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {len(all_results)}ê°œ í‚¤ì›Œë“œ ì²˜ë¦¬ë¨")
        return all_results
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ë³„ ë³‘ë ¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return [KeywordBasicData(
            keyword=keyword,
            search_volume=0,
            category="ë¶„ì„ ì‹¤íŒ¨"
        ) for keyword in keywords]


def extract_product_id_from_link(link: str) -> str:
    """ìƒí’ˆ ë§í¬ì—ì„œ productId ì¶”ì¶œ (rank_trackingê³¼ ë™ì¼)"""
    if not link:
        return ""
    
    patterns = [
        r'https?://shopping\.naver\.com/catalog/(\d+)',
        r'https?://smartstore\.naver\.com/[^/]+/products/(\d+)',
        r'/catalog/(\d+)',
        r'/products/(\d+)',
        r'productId=(\d+)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, link)
        if match:
            return match.group(1)
    
    return ""


def extract_price(price_str: str) -> int:
    """ê°€ê²© ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ"""
    if not price_str:
        return 0
    numbers = re.findall(r'\d+', str(price_str))
    if numbers:
        return int(''.join(numbers))
    return 0


def collect_product_names_for_keyword(keyword: str, max_count: int = 40) -> List[Dict[str, Any]]:
    """
    í‚¤ì›Œë“œë¡œ ìƒí’ˆëª… ìˆ˜ì§‘ (1~40ìœ„)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
    Returns:
        List[Dict]: ìƒí’ˆëª… ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        logger.debug(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì‹œì‘: {keyword} (ìµœëŒ€ {max_count}ê°œ)")
        
        from src.vendors.naver.client_factory import NaverClientFactory
        client = NaverClientFactory.get_shopping_client()
        if not client:
            logger.warning(f"ì‡¼í•‘ í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ: {keyword}")
            return []
        
        # ë„¤ì´ë²„ ì‡¼í•‘ APIë¡œ ê²€ìƒ‰
        response = client.search_products(
            query=keyword,
            display=min(max_count, 100),  # ìµœëŒ€ 100ê°œ
            start=1,
            sort="sim"  # ì •í™•ë„ ìˆœ
        )
        
        if not response or not hasattr(response, 'items') or not response.items:
            logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {keyword}")
            return []
        
        # ìƒí’ˆëª… ì •ë³´ ì¶”ì¶œ
        product_names = []
        for idx, item in enumerate(response.items):
            if idx >= max_count:
                break
                
            # HTML íƒœê·¸ ì œê±°
            clean_title = item.title.replace('<b>', '').replace('</b>', '') if hasattr(item, 'title') and item.title else ''
            if not clean_title:
                continue
                
            # ì¹´í…Œê³ ë¦¬ ê²½ë¡œ êµ¬ì„±
            categories = []
            for cat_field in ['category1', 'category2', 'category3', 'category4']:
                if hasattr(item, cat_field):
                    category_value = getattr(item, cat_field)
                    if category_value and category_value.strip():
                        categories.append(category_value.strip())
                    else:
                        break
            
            category_path = ' > '.join(categories) if categories else ''
            
            product_names.append({
                'rank': idx + 1,
                'title': clean_title.strip(),
                'keyword': keyword,
                'price': extract_price(getattr(item, 'lprice', '')),
                'mall_name': getattr(item, 'mallName', ''),
                'category': category_path,
                'product_id': extract_product_id_from_link(getattr(item, 'link', '')),
                'image_url': getattr(item, 'image', ''),
                'link': getattr(item, 'link', '')
            })
        
        logger.debug(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì™„ë£Œ: {keyword} - {len(product_names)}ê°œ")
        return product_names
        
    except Exception as e:
        logger.error(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì‹¤íŒ¨: {keyword}: {e}")
        return []


def collect_product_names_for_keywords(keywords: List[str], max_count_per_keyword: int = 40) -> List[Dict[str, Any]]:
    """
    ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ìƒí’ˆëª… ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        max_count_per_keyword: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
    Returns:
        List[Dict]: ì¤‘ë³µ ì œê±°ëœ ìƒí’ˆëª… ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        logger.info(f"ìƒí’ˆëª… ì¼ê´„ ìˆ˜ì§‘ ì‹œì‘: {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        
        all_products = []
        
        # ê° í‚¤ì›Œë“œë³„ë¡œ ìƒí’ˆëª… ìˆ˜ì§‘
        for keyword in keywords:
            products = collect_product_names_for_keyword(keyword, max_count_per_keyword)
            all_products.extend(products)
        
        logger.info(f"ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_products)}ê°œ (ì¤‘ë³µ ì œê±° ì „)")
        
        # ì¤‘ë³µ ì œê±° (ìƒí’ˆ ì œëª© ê¸°ì¤€)
        unique_products = []
        seen_titles = set()
        
        for product in all_products:
            title = product['title'].strip().lower()
            if title not in seen_titles:
                seen_titles.add(title)
                unique_products.append(product)
        
        # ìˆœìœ„ ì¬ì •ë ¬ (í‚¤ì›Œë“œë³„ í‰ê·  ìˆœìœ„ ê¸°ì¤€)
        title_ranks = {}
        title_keywords = {}
        
        for product in all_products:
            title = product['title'].strip().lower()
            rank = product['rank']
            keyword = product['keyword']
            
            if title not in title_ranks:
                title_ranks[title] = []
                title_keywords[title] = set()
            
            title_ranks[title].append(rank)
            title_keywords[title].add(keyword)
        
        # í‰ê·  ìˆœìœ„ë¡œ ì •ë ¬
        for product in unique_products:
            title = product['title'].strip().lower()
            product['avg_rank'] = sum(title_ranks[title]) / len(title_ranks[title])
            product['keywords_found_in'] = list(title_keywords[title])
            product['keyword_count'] = len(title_keywords[title])
        
        # í‰ê·  ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
        unique_products.sort(key=lambda x: x['avg_rank'])
        
        # ìµœì¢… ìˆœìœ„ ë¶€ì—¬
        for idx, product in enumerate(unique_products):
            product['final_rank'] = idx + 1
        
        logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(unique_products)}ê°œ (ì œê±°ëœ ì¤‘ë³µ: {len(all_products) - len(unique_products)}ê°œ)")
        return unique_products
        
    except Exception as e:
        logger.error(f"ìƒí’ˆëª… ì¼ê´„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []


def get_keywords_category(keyword_data_list: List[KeywordBasicData],
                         max_workers: int = 2,
                         stop_check: Optional[Callable[[], bool]] = None,
                         progress_callback: Optional[Callable[[int, int, str], None]] = None) -> List[KeywordBasicData]:
    """
    í‚¤ì›Œë“œë“¤ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì¡°íšŒ (adapters ì—­í• : ë²¤ë” í˜¸ì¶œ + ì •ê·œí™”)
    
    Args:
        keyword_data_list: ì›”ê²€ìƒ‰ëŸ‰ì´ í¬í•¨ëœ KeywordBasicData ë¦¬ìŠ¤íŠ¸
        max_workers: ìµœëŒ€ ë™ì‹œ ì‘ì—… ìˆ˜ (ê¸°ë³¸ 2ê°œ)
        stop_check: ì¤‘ë‹¨ í™•ì¸ í•¨ìˆ˜
        progress_callback: ì§„í–‰ë¥  ì½œë°±
        
    Returns:
        List[KeywordBasicData]: ì›”ê²€ìƒ‰ëŸ‰ + ì¹´í…Œê³ ë¦¬ê°€ í¬í•¨ëœ ì •ê·œí™”ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if not keyword_data_list:
        return []
    
    # ë‹¨ì¼ í‚¤ì›Œë“œì¸ ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬ ì—†ì´ ì§ì ‘ í˜¸ì¶œ
    if len(keyword_data_list) == 1:
        try:
            keyword_data = keyword_data_list[0]
            category = get_keyword_category(keyword_data.keyword)
            return [KeywordBasicData(
                keyword=keyword_data.keyword,
                search_volume=keyword_data.search_volume,
                category=category
            )]
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ '{keyword_data_list[0].keyword}': {e}")
            return [KeywordBasicData(
                keyword=keyword_data_list[0].keyword,
                search_volume=keyword_data_list[0].search_volume,
                category="ë¶„ì„ ì‹¤íŒ¨"
            )]
    
    # ì—¬ëŸ¬ í‚¤ì›Œë“œëŠ” ë³‘ë ¬ ì²˜ë¦¬
    try:
        logger.info(f"ğŸ”„ ë³‘ë ¬ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œì‘: {len(keyword_data_list)}ê°œ í‚¤ì›Œë“œ, {max_workers}ê°œ ì›Œì»¤")
        
        # ë ˆì´íŠ¸ ë¦¬ë¯¸í„° ì„¤ì • (ì¹´í…Œê³ ë¦¬ ì¡°íšŒë„ ë™ì¼í•œ ì†ë„)
        api_limiter = rate_limiter_manager.get_limiter("naver_category_api", calls_per_second=1.0)
        
        # ë³‘ë ¬ ì²˜ë¦¬ê¸° ìƒì„±
        processor = ParallelAPIProcessor(max_workers=max_workers, rate_limiter=api_limiter)
        
        # ë‹¨ì¼ í‚¤ì›Œë“œ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
        def process_single_keyword_category(keyword_data: KeywordBasicData) -> KeywordBasicData:
            try:
                category = get_keyword_category(keyword_data.keyword)
                return KeywordBasicData(
                    keyword=keyword_data.keyword,
                    search_volume=keyword_data.search_volume,
                    category=category
                )
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ '{keyword_data.keyword}': {e}")
                return KeywordBasicData(
                    keyword=keyword_data.keyword,
                    search_volume=keyword_data.search_volume,
                    category="ë¶„ì„ ì‹¤íŒ¨"
                )
        
        # ì§„í–‰ë¥  ì½œë°± ë˜í¼ (í‚¤ì›Œë“œëª… í¬í•¨)
        def wrapped_progress_callback(current: int, total: int, current_item: KeywordBasicData):
            if progress_callback:
                message = f"ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì¤‘ '{current_item.keyword}' ({current}/{total})"
                progress_callback(current, total, message)
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        batch_results = processor.process_batch(
            func=process_single_keyword_category,
            items=keyword_data_list,
            stop_check=stop_check,
            progress_callback=lambda current, total, msg: wrapped_progress_callback(current, total, keyword_data_list[current-1] if current > 0 and current <= len(keyword_data_list) else None)
        )
        
        # ê²°ê³¼ ì •ë¦¬
        results = []
        for keyword_data, result, error in batch_results:
            if error is None and result is not None:
                results.append(result)
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ ë°ì´í„°ì— ì‹¤íŒ¨ í‘œì‹œ
                results.append(KeywordBasicData(
                    keyword=keyword_data.keyword,
                    search_volume=keyword_data.search_volume,
                    category="ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨"
                ))
        
        logger.info(f"âœ… ë³‘ë ¬ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì™„ë£Œ: {len(results)}ê°œ í‚¤ì›Œë“œ")
        return results
        
    except Exception as e:
        logger.error(f"ë³‘ë ¬ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°ì´í„°ì— ë¹ˆ ì¹´í…Œê³ ë¦¬ ì¶”ê°€í•´ì„œ ë°˜í™˜
        return [KeywordBasicData(
            keyword=kd.keyword,
            search_volume=kd.search_volume,
            category="ì¡°íšŒ ì‹¤íŒ¨"
        ) for kd in keyword_data_list]