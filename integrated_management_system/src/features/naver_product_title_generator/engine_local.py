"""
ë„¤ì´ë²„ ìƒí’ˆëª… ìƒì„±ê¸° ë¡œì»¬ ì—”ì§„
ìˆœìˆ˜ í•¨ìˆ˜ ê¸°ë°˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (I/O ì—†ìŒ)
"""
from typing import List, Dict, Any, Optional
import re
import math
from collections import Counter

from .models import KeywordBasicData, GeneratedTitle


# AI í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ (ê³µìš© - ëª¨ë“  í”„ë¡¬í”„íŠ¸ì— ìë™ ì¶”ê°€)
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ì‡¼í•‘ ìƒí’ˆëª… ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.  
ì•„ë˜ ìƒí’ˆëª…ì„ ë¶„ì„í•´, ì‚¬ëŒë“¤ì´ ì‹¤ì œ ê²€ìƒ‰í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.  
ê²°ê³¼ëŠ” ë„¤ì´ë²„ ì›”ê°„ ê²€ìƒ‰ëŸ‰ API ë¹„êµìš©ì´ë©°, ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ê³µìš©ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.

ğŸ”¸ í•„ìˆ˜ ì¶œë ¥ í˜•ì‹ (ì‚¬ìš©ì ê·œì¹™ê³¼ ìƒê´€ì—†ì´ ë°˜ë“œì‹œ ì¤€ìˆ˜)
- í‚¤ì›Œë“œë§Œ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ í•œ ì¤„ì— ì¶œë ¥
- ì˜ˆì‹œ: ìŠ¤ë§ˆíŠ¸í°, ê°¤ëŸ­ì‹œ, ì•„ì´í°, íœ´ëŒ€í°, ëª¨ë°”ì¼  
- ì„¤ëª…ì´ë‚˜ ê¸°íƒ€ ë¬¸ì¥ ì ˆëŒ€ ê¸ˆì§€
- single + bigram + trigram í•©ì‚° 300ê°œ ì´ìƒì˜ í‚¤ì›Œë“œ ìƒì„±"""

# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë‚´ìš© (ì‚¬ìš©ìì—ê²Œ í‘œì‹œìš©)
DEFAULT_AI_PROMPT = """[ê·œì¹™]

1. ë¸Œëœë“œëª… ì œê±°  
- ëª¨ë“  ë¸Œëœë“œëª… ì‚­ì œ
- ë³´í†µ ìƒí’ˆëª… ì•ì— ë‹¨ì–´ê°€ ë¸Œëœë“œëª…ì¼ í™•ë¥ ì´ 90% ì´ë‹¤.

2. ê·œê²©Â·ë‹¨ìœ„ ì œê±°  
- g, kg, ml, ê°œ, ê°œì…, ë°•ìŠ¤, ì„¸íŠ¸, %, p, S, M, L, ì†Œí˜•, ì¤‘í˜•, ëŒ€í˜• ë“± ì œì™¸.  

3. ë¶ˆí•„ìš” ë‹¨ì–´ ì œê±°  
- ì¸ê¸°, íŠ¹ê°€, ì‹ ìƒ, í• ì¸, ë¬´ë£Œë°°ì†¡, ë©‹ì§„, ì´ìœ, ì„¸ë ¨ëœ, ê°€ì„±ë¹„ ì™€ ê°™ì€ ì£¼ê´€ì ì¸ ë‹¨ì–´, ì œí’ˆì„ ê¾¸ë©°ì£¼ëŠ” ë‹¨ì–´ ì œì™¸.  

4. ì¸ì¦/ê¸°ê´€ì–´(HACCP ë“±) ì œê±°

5. ë‚˜ë¨¸ì§€ ìƒí’ˆëª… í˜•íƒœì†Œ ë¶„ë¥˜  
- 1.ëŒ€ìƒ: ì˜ˆ) ê°•ì•„ì§€, ë°˜ë ¤ê²¬, ì• ê²¬, ë‚¨ì, ë‚¨ì„±, ì—¬ì, ì—¬ì„± ë“±...
- 2.íš¨ëŠ¥/ìƒí™©: ì˜ˆ)ì¹˜ì„ì œê±°, ì…ëƒ„ìƒˆì œê±°, ì¹˜ì•„ê´€ë¦¬, ì–‘ì¹˜, ìŠ¤ì¼€ì¼ë§, ì´ê°ˆì´, êµ¬ê°•ì¼€ì–´ ë“±...  
- 3.í˜•íƒœ  
- 4.ì£¼ì›ë£Œ
- 5.ì œí’ˆ ê³ ìœ ëª…ì‚¬

6. í‚¤ì›Œë“œ ìƒì„± ê·œì¹™

1) ì‹±ê¸€(single_keywords)  
- ì˜¤ì§ íŠ¹ì •ìƒí’ˆëª…ì„ ì§€ì¹­í•˜ëŠ” ë‹¨ì–´ë§Œ ë„£ëŠ”ë‹¤.
- ëŒ€ìƒì€ ë„£ì§€ ì•ŠëŠ”ë‹¤. ì˜ˆ)ê°•ì•„ì§€, ë°˜ë ¤ê²¬, ì• ê²¬, ë‚¨ì„±, ì—¬ì„± ë“±... 

2) ë¹…ëŒ(bigram_keywords)  
- ìœ„ 5ìš”ì†Œ ì¤‘ 2ê°œ ì¡°í•©.  
- ëŒ€ìƒ+íš¨ëŠ¥/ìƒí™©, ëŒ€ìƒ+í˜•íƒœ, ëŒ€ìƒ+ì œí’ˆê³ ìœ ëª…ì‚¬, í˜•íƒœ+ì œí’ˆê³ ìœ ëª…ì‚¬, ì£¼ì›ë£Œ+ì œí’ˆê³ ìœ ëª…ì‚¬  
- ëŒ€ìƒì´ ì—¬ëŸ¬ëª…ì¼ ê²½ìš° ëŒ€ìƒ+ì œí’ˆê³ ìœ ëª…ì‚¬ ì¡°í•©ì„ ì£¼ë ¥ìœ¼ë¡œ ìƒì„±
- ê°€ëŠ¥í•œ í•œ ë§ì´ ìƒì„±  

3) íŠ¸ë¦¬ê·¸ë¨(trigram_keywords)  
- ìœ„ 5ìš”ì†Œ ì¤‘ 3ê°œ ì¡°í•© 
- ëŒ€ìƒ+íš¨ëŠ¥/ìƒí™©+ì œí’ˆê³ ìœ ëª…ì‚¬, ëŒ€ìƒ+í˜•íƒœ+ì œí’ˆê³ ìœ ëª…ì‚¬, ëŒ€ìƒ+ì£¼ì›ë£Œ+ì œí’ˆê³ ìœ ëª…ì‚¬, ì˜ë¯¸ ëª…í™• ê²€ìƒ‰ ê°€ëŠ¥ì„± ë†’ì€ í•œêµ­ì–´ ì–´ìˆœìœ¼ë¡œ ì¡°í•©ë„ í¬í•¨  

7. ê³µí†µ  
- ì‹¤ì œ í•œêµ­ì–´ ê²€ìƒ‰ ì–´ìˆœìœ¼ë¡œ ì–´ìƒ‰í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ì–´ ì ìš©.  
- ë¶™ì„Â·ë„ì–´ì“°ê¸°ëŠ” ë™ì¼ í‚¤ì›Œë“œë¡œ ê°„ì£¼, í•œ í˜•íƒœë§Œ ì¶œë ¥.  
- ì¡°ê±´ ë¯¸ë‹¬ ì‹œ ê°œìˆ˜ í™•ë³´ ìœ„í•´ ê·œì¹™ ì™„í™” ê°€ëŠ¥.  

8. ì¶œë ¥ í˜•ì‹  
- í‚¤ì›Œë“œë§Œ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„.  
- single + bigram + trigram í•©ì‚° 300ê°œ ì´ìƒ.  
- ì¤‘ë³µ ì œê±°(ë¶™ì„Â·ë„ì–´ì“°ê¸° ë™ì¼ í‚¤ì›Œë“œë„ ì¤‘ë³µ ì œê±°).  
- ì„¤ëª…Â·ê¸°íƒ€ ë¬¸ì¥ ê¸ˆì§€."""


def extract_keywords_from_product_name(product_name: str) -> List[str]:
    """ì œí’ˆëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì•ìœ¼ë¡œ ì‚¬ìš©í•  í•¨ìˆ˜)"""
    # í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ì •ë¦¬
    cleaned = re.sub(r'[^\w\sê°€-í£]', ' ', product_name)
    
    keywords = set()
    words = cleaned.split()
    
    # 1. ê°œë³„ ë‹¨ì–´ë“¤
    for word in words:
        if len(word) >= 2:
            keywords.add(word.lower())
    
    # 2. 2ê¸€ì ì¡°í•©
    if len(words) > 1:
        for i in range(len(words) - 1):
            combined = f"{words[i]} {words[i+1]}".lower()
            keywords.add(combined)
    
    # 3. ì „ì²´ ì¡°í•©
    if len(words) <= 3:  # 3ë‹¨ì–´ ì´í•˜ì¼ ë•Œë§Œ
        keywords.add(product_name.lower())
    
    return list(keywords)


def calculate_seo_score(title: str, keywords: List[str]) -> float:
    """SEO ì ìˆ˜ ê³„ì‚° (ì•ìœ¼ë¡œ ì‚¬ìš©í•  í•¨ìˆ˜)"""
    score = 50.0  # ê¸°ë³¸ ì ìˆ˜
    
    # í‚¤ì›Œë“œ í¬í•¨ë„
    for keyword in keywords:
        if keyword.lower() in title.lower():
            score += 10
    
    # ê¸¸ì´ ì ì •ì„±
    length = len(title)
    if 20 <= length <= 40:
        score += 20
    elif length < 20:
        score -= 10
    elif length > 50:
        score -= 15
    
    return min(score, 100.0)


def generate_title_variations(keywords: List[str], original_product: str) -> List[str]:
    """í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ìƒí’ˆëª… ë³€í˜• ìƒì„± (ì•ìœ¼ë¡œ êµ¬í˜„í•  í•¨ìˆ˜)"""
    variations = set()
    
    # ê¸°ë³¸ íŒ¨í„´ë“¤
    patterns = [
        "{product} {keyword}",
        "{keyword} {product}",
        "{product} {keyword} ì¶”ì²œ",
        "{keyword} ì „ë¬¸ {product}",
        "ì¸ê¸° {keyword} {product}"
    ]
    
    # ê° í‚¤ì›Œë“œì™€ íŒ¨í„´ ì¡°í•©
    for keyword in keywords[:5]:  # ìƒìœ„ 5ê°œë§Œ
        for pattern in patterns:
            title = pattern.format(product=original_product, keyword=keyword)
            if len(title) <= 50:  # 50ì ì œí•œ
                variations.add(title)
    
    return list(variations)[:10]  # ìµœëŒ€ 10ê°œ


def build_ai_prompt(product_titles: List[str], custom_prompt: Optional[str] = None) -> str:
    """
    AI ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        product_titles: ë¶„ì„í•  ìƒí’ˆëª… ë¦¬ìŠ¤íŠ¸
        custom_prompt: ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ (Noneì´ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        
    Returns:
        str: ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
    """
    # ìƒí’ˆëª… ëª©ë¡ í…ìŠ¤íŠ¸ ìƒì„±
    titles_text = "\n".join([f"- {title}" for title in product_titles])
    
    if custom_prompt:
        # ê³µìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ + ìƒí’ˆëª… ëª©ë¡
        return f"{SYSTEM_PROMPT}\n\n{custom_prompt}\n\nìƒí’ˆëª… ëª©ë¡:\n{titles_text}"
    else:
        # ê³µìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ + ìƒí’ˆëª… ëª©ë¡
        return f"{SYSTEM_PROMPT}\n\n{DEFAULT_AI_PROMPT}\n\nìƒí’ˆëª… ëª©ë¡:\n{titles_text}"


def parse_ai_keywords_response(ai_response: str) -> List[str]:
    """
    AI ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        ai_response: AIê°€ ë°˜í™˜í•œ í…ìŠ¤íŠ¸
        
    Returns:
        List[str]: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = []
    
    # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‚˜ëˆ„ê³  ê° ì¤„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    lines = ai_response.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith('#') or line.startswith('-'):
            continue
            
        # ì‰¼í‘œë¡œ êµ¬ë¶„
        if ',' in line:
            parts = line.split(',')
            for part in parts:
                keyword = part.strip()
                # "+" ê¸°í˜¸ ì œê±° (ì˜ˆ: "ê°•ì•„ì§€+ì˜¤ë˜ë¨¹ëŠ”+ê°œê»Œ" -> "ê°•ì•„ì§€ì˜¤ë˜ë¨¹ëŠ”ê°œê»Œ")
                keyword = keyword.replace('+', '')
                if keyword and len(keyword) >= 2:
                    keywords.append(keyword)
        else:
            # ì‰¼í‘œê°€ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í‚¤ì›Œë“œë¡œ
            line = line.replace('+', '')  # "+" ê¸°í˜¸ ì œê±°
            if len(line) >= 2:
                keywords.append(line)
    
    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
    unique_keywords = []
    seen = set()
    
    for keyword in keywords:
        keyword_lower = keyword.lower().strip()
        if keyword_lower not in seen:
            seen.add(keyword_lower)
            unique_keywords.append(keyword.strip())
    
    return unique_keywords


def filter_keywords_by_search_volume(keywords: List[KeywordBasicData], min_volume: int = 100) -> List[KeywordBasicData]:
    """
    ê²€ìƒ‰ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œ í•„í„°ë§
    
    Args:
        keywords: í‚¤ì›Œë“œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        min_volume: ìµœì†Œ ì›”ê°„ ê²€ìƒ‰ëŸ‰
        
    Returns:
        List[KeywordBasicData]: í•„í„°ë§ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    filtered = [kw for kw in keywords if kw.search_volume >= min_volume]
    
    # ê²€ìƒ‰ëŸ‰ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    filtered.sort(key=lambda x: x.search_volume, reverse=True)
    
    return filtered


def filter_keywords_by_category(keywords: List[KeywordBasicData], target_category: str) -> List[KeywordBasicData]:
    """
    ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œ í•„í„°ë§ (1ë‹¨ê³„ì—ì„œ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ì™€ ë§¤ì¹­)
    
    Args:
        keywords: í•„í„°ë§í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        target_category: 1ë‹¨ê³„ì—ì„œ ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬
        
    Returns:
        List[KeywordBasicData]: ë§¤ì¹­ë˜ëŠ” ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    if not keywords or not target_category:
        return keywords
    
    # % ë¶€ë¶„ ì œê±°
    target_clean = target_category.split('(')[0].strip() if '(' in target_category else target_category.strip()
    
    # ë””ë²„ê¹…ìš© ë¡œê·¸
    from src.foundation.logging import get_logger
    logger = get_logger("features.naver_product_title_generator.engine_local")
    logger.info(f"ğŸ¯ í•„í„°ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: '{target_clean}'")
    
    filtered = []
    for kw in keywords:
        if not kw.category:
            logger.debug(f"  âŒ '{kw.keyword}' - ì¹´í…Œê³ ë¦¬ ì—†ìŒ")
            continue
            
        # í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ë„ % ë¶€ë¶„ ì œê±°
        kw_clean = kw.category.split('(')[0].strip() if '(' in kw.category else kw.category.strip()
        
        # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ë¡œì§ (ì›ë³¸ ë¬¸ìì—´ë¡œ ë¹„êµ)
        is_match = is_category_match(target_clean, kw_clean)
        logger.info(f"  {'âœ…' if is_match else 'âŒ'} '{kw.keyword}' - '{kw_clean}' {'ë§¤ì¹­!' if is_match else 'ë¶ˆì¼ì¹˜'}")
        
        if is_match:
            filtered.append(kw)
    
    # ê²€ìƒ‰ëŸ‰ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    filtered.sort(key=lambda x: x.search_volume, reverse=True)
    
    logger.info(f"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(keywords)}ê°œ ì¤‘ {len(filtered)}ê°œ ë§¤ì¹­")
    
    return filtered


def is_category_match(target_category: str, keyword_category: str) -> bool:
    """
    ë‘ ì¹´í…Œê³ ë¦¬ê°€ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
    
    Args:
        target_category: 1ë‹¨ê³„ ì„ íƒ ì¹´í…Œê³ ë¦¬
        keyword_category: í‚¤ì›Œë“œì˜ ì¹´í…Œê³ ë¦¬
        
    Returns:
        bool: ë§¤ì¹­ ì—¬ë¶€
    """
    if not target_category or not keyword_category:
        return False
    
    # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë¹„êµ
    target_lower = target_category.lower()
    keyword_lower = keyword_category.lower()
    
    # ì •í™•íˆ ê°™ì€ ê²½ìš°
    if target_lower == keyword_lower:
        return True
    
    # ì¹´í…Œê³ ë¦¬ ê²½ë¡œ ë¶„ë¦¬ (> ê¸°ì¤€)
    target_parts = [part.strip() for part in target_lower.split('>') if part.strip()]
    keyword_parts = [part.strip() for part in keyword_lower.split('>') if part.strip()]
    
    if not target_parts or not keyword_parts:
        return False
    
    # ë‘ ì¹´í…Œê³ ë¦¬ì˜ ìµœì†Œ ê¸¸ì´ê¹Œì§€ ë¹„êµ (ì „ì²´ ê¹Šì´ ë¹„êµ)
    min_depth = min(len(target_parts), len(keyword_parts))
    
    for i in range(min_depth):
        # ê° ë‹¨ê³„ì—ì„œ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ False
        if target_parts[i] != keyword_parts[i]:
            return False
    
    # ëª¨ë“  ë‹¨ê³„ê°€ ì¼ì¹˜í•˜ë©´ True
    return True


def normalize_keyword_for_comparison(keyword: str) -> str:
    """
    í‚¤ì›Œë“œ ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ìš© (ì†Œë¬¸ì ë³€í™˜)
    
    Args:
        keyword: ì›ë³¸ í‚¤ì›Œë“œ
        
    Returns:
        str: ì¤‘ë³µ ì²´í¬ìš© ì •ê·œí™”ëœ í‚¤ì›Œë“œ (ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜)
    """
    if not keyword:
        return ""
    
    # 1. ì•ë’¤ ê³µë°± ì œê±°
    cleaned = keyword.strip()
    
    # 2. ë‚´ë¶€ ê³µë°± ì œê±° (ë„¤ì´ë²„ APIëŠ” ë„ì–´ì“°ê¸° ì—†ëŠ” í˜•íƒœë¡œë§Œ ì¸ì‹)
    normalized = cleaned.replace(" ", "")
    
    # 3. ì†Œë¬¸ìë¡œ ë³€í™˜ (ì¤‘ë³µ ì²´í¬ìš©)
    normalized = normalized.lower()
    
    return normalized


def normalize_keyword_for_api(keyword: str) -> str:
    """
    í‚¤ì›Œë“œ ì •ê·œí™” - ë„¤ì´ë²„ API í˜¸ì¶œìš© (ëŒ€ë¬¸ì ë³€í™˜)
    
    Args:
        keyword: ì›ë³¸ í‚¤ì›Œë“œ
        
    Returns:
        str: API í˜¸ì¶œìš© ì •ê·œí™”ëœ í‚¤ì›Œë“œ (ê³µë°± ì œê±°, ëŒ€ë¬¸ì ë³€í™˜)
    """
    if not keyword:
        return ""
    
    # 1. ì•ë’¤ ê³µë°± ì œê±°
    cleaned = keyword.strip()
    
    # 2. ë‚´ë¶€ ê³µë°± ì œê±° (ë„¤ì´ë²„ APIëŠ” ë„ì–´ì“°ê¸° ì—†ëŠ” í˜•íƒœë¡œë§Œ ì¸ì‹)
    normalized = cleaned.replace(" ", "")
    
    # 3. ëŒ€ë¬¸ìë¡œ ë³€í™˜ (ë„¤ì´ë²„ API ìš”êµ¬ì‚¬í•­)
    normalized = normalized.upper()
    
    return normalized


def deduplicate_keywords(keywords: List[str]) -> List[str]:
    """
    í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ ì œê±°
    "ê°•ì•„ì§€ê°„ì‹"ê³¼ "ê°•ì•„ì§€ ê°„ì‹"ì„ ê°™ì€ í‚¤ì›Œë“œë¡œ ì²˜ë¦¬
    
    Args:
        keywords: ì›ë³¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[str]: ì¤‘ë³µ ì œê±°ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ í˜•íƒœ ìœ ì§€)
    """
    if not keywords:
        return []
    
    seen_normalized = set()  # ì •ê·œí™”ëœ í‚¤ì›Œë“œ ì €ì¥ìš©
    unique_keywords = []     # ì›ë³¸ í˜•íƒœì˜ í‚¤ì›Œë“œ ì €ì¥ìš©
    keyword_mapping = {}     # ì •ê·œí™”ëœ í‚¤ì›Œë“œ -> ì›ë³¸ í‚¤ì›Œë“œ ë§¤í•‘
    
    for keyword in keywords:
        if not keyword or not keyword.strip():
            continue
            
        normalized = normalize_keyword_for_comparison(keyword)
        
        if normalized not in seen_normalized:
            seen_normalized.add(normalized)
            # ì›ë³¸ í‚¤ì›Œë“œ í˜•íƒœë¥¼ ë³´ì¡´ (ì²« ë²ˆì§¸ë¡œ ë‚˜ì˜¨ í˜•íƒœ ì‚¬ìš©)
            unique_keywords.append(keyword.strip())
            keyword_mapping[normalized] = keyword.strip()
    
    return unique_keywords


def calculate_keyword_score(keyword_data: KeywordBasicData) -> float:
    """
    í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚° (ê²€ìƒ‰ëŸ‰ ê¸°ë°˜)
    
    Args:
        keyword_data: í‚¤ì›Œë“œ ë°ì´í„°
        
    Returns:
        float: í‚¤ì›Œë“œ ì ìˆ˜ (0-100)
    """
    # ê¸°ë³¸ ì ìˆ˜ëŠ” ê²€ìƒ‰ëŸ‰ ê¸°ë°˜
    volume_score = min(keyword_data.search_volume / 1000 * 50, 70)  # ìµœëŒ€ 70ì 
    
    # í‚¤ì›Œë“œ ê¸¸ì´ ë³´ë„ˆìŠ¤ (2-4ê¸€ìê°€ ì ì •)
    length_bonus = 0
    length = len(keyword_data.keyword)
    if 2 <= length <= 4:
        length_bonus = 20
    elif length == 5:
        length_bonus = 10
    elif length == 6:
        length_bonus = 5
    
    total_score = volume_score + length_bonus
    return min(total_score, 100.0)